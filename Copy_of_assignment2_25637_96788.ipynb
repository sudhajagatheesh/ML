{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhajagatheesh/ML/blob/master/Copy_of_assignment2_25637_96788.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2zLC_h4PrhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUqztWybzRlO"
      },
      "outputs": [],
      "source": [
        "# loading datasets\n",
        "annual_report_df = pd.read_json('AnnualReport.json')\n",
        "apartment_df = pd.read_json('Apartment.json')\n",
        "housing_association_df = pd.read_json('HousingAssociation.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge Apartment data with Housing Association data\n",
        "apartment_full_df = pd.merge(apartment_df, housing_association_df, left_on='housing_association_org_number', right_on='org_number', how='left')\n",
        "apartment_full_df.head()"
      ],
      "metadata": {
        "id": "c2qUZHa3pCTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique values in 'association_tax_liability' to understand what non-numeric values it contains\n",
        "print(annual_report_df['association_tax_liability'].unique())"
      ],
      "metadata": {
        "id": "PEVKUx0tljIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'association_tax_liability' column to numeric, setting errors='coerce' to handle non-numeric data\n",
        "annual_report_df['association_tax_liability'] = pd.to_numeric(annual_report_df['association_tax_liability'], errors='coerce')\n",
        "\n",
        "# You may need to do this for other columns as well depending on their content\n",
        "columns_to_convert = ['long_term_debt_other', 'long_term_real_estate_debt', 'total_living_area', 'total_loan', 'savings']\n",
        "for col in columns_to_convert:\n",
        "    annual_report_df[col] = pd.to_numeric(annual_report_df[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "_3G_QqKtllLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate Annual Reports by Housing Association again\n",
        "aggregated_annual_reports = annual_report_df.groupby('org_number').agg({\n",
        "    'association_tax_liability': 'mean',\n",
        "    'long_term_debt_other': 'mean',\n",
        "    'long_term_real_estate_debt': 'mean',\n",
        "    'number_of_rental_units': 'sum',\n",
        "    'number_of_units': 'sum',\n",
        "    'total_commercial_area': 'sum',\n",
        "    'total_living_area': 'mean',\n",
        "    'total_loan': 'mean',\n",
        "    'total_plot_area': 'sum',\n",
        "    'total_rental_area': 'sum',\n",
        "    'savings': 'mean',\n",
        "    'fiscal_year' : 'max',\n",
        "    'housing_coop_id': 'first',\n",
        "    'plot_is_leased': 'max'\n",
        "}).reset_index()\n",
        "\n",
        "# Merge the result with Aggregated Annual Reports\n",
        "final_df = pd.merge(apartment_full_df, aggregated_annual_reports, left_on='housing_association_org_number', right_on='org_number', how='left')\n",
        "\n",
        "# Check the final merged DataFrame\n",
        "print(\"\\nFinal Merged DataFrame:\")\n",
        "print(final_df.head())\n"
      ],
      "metadata": {
        "id": "oH5y1XhtlyjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming final_df is your DataFrame from which you want to remove duplicate columns\n",
        "def remove_duplicate_columns(df):\n",
        "    # Create an empty dictionary to store the comparison of columns\n",
        "    duplicates = {}\n",
        "\n",
        "    # Iterate over all columns in DataFrame\n",
        "    for i, col1 in enumerate(df.columns):\n",
        "        for j, col2 in enumerate(df.columns[i + 1:]):  # Compare with subsequent columns\n",
        "            # If columns have the same content\n",
        "            if df[col1].equals(df[col2]):\n",
        "                # If col2 is not already identified as a duplicate of some other column\n",
        "                if col2 not in duplicates:\n",
        "                    duplicates[col2] = col1  # Map col2 as a duplicate of col1\n",
        "\n",
        "    # Drop duplicate columns from the DataFrame\n",
        "    df = df.drop(columns=duplicates.keys())\n",
        "    return df\n",
        "\n",
        "# Remove duplicate columns\n",
        "final_df = remove_duplicate_columns(final_df)\n",
        "\n",
        "# Print the DataFrame to confirm removal of duplicates\n",
        "print(\"DataFrame after removing duplicate columns:\")\n",
        "print(final_df.head())\n",
        "final_df.to_excel('final_condensed_data.xlsx', index=False, engine='openpyxl')\n"
      ],
      "metadata": {
        "id": "5fXtk0iGswzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of rows before removal:\", final_df.shape[0])\n",
        "\n",
        "# Remove rows where 'object_type' is either 'Twin House' or 'Rowhouse'\n",
        "final_df = final_df[~final_df['object_type'].isin(['Twin House', 'Rowhouse'])]\n",
        "\n",
        "# Print the number of rows after removal to confirm rows were deleted\n",
        "print(\"Number of rows after removal:\", final_df.shape[0])"
      ],
      "metadata": {
        "id": "zVrebp69r7fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_sell_price = final_df[final_df['sell_price'] > 0]\n",
        "\n",
        "# Dataset where sell_price is zero or NaN (assuming sell_price has been properly cleaned and is numeric)\n",
        "df_without_sell_price = final_df[(final_df['sell_price'] == 0) | (final_df['sell_price'].isna())]\n",
        "\n",
        "# Optionally, you can print the first few rows to verify\n",
        "print(\"Dataset With Sell Price:\")\n",
        "print(df_with_sell_price.head())\n",
        "df_with_sell_price.to_json('df_with_sell_price.json', orient='records', lines=True)\n",
        "print(\"\\nDataset Without Sell Price:\")\n",
        "print(df_without_sell_price.head())\n",
        "df_without_sell_price.to_json('df_without_sell_price.json', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "ww3YVHEVtefD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df_with_sell_price is your DataFrame\n",
        "relevant_columns = [col for col in df_with_sell_price.columns if col != 'sell_price' and df_with_sell_price[col].dtype != 'bool']\n",
        "num_columns = len(relevant_columns)\n",
        "\n",
        "# Dynamic adjustment of the grid layout for subplots\n",
        "num_rows = (num_columns // 3) + (num_columns % 3 > 0)\n",
        "fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, 5 * num_rows), squeeze=False)\n",
        "axs = axs.flatten()  # Flatten the array to simplify indexing\n",
        "\n",
        "# Iterate over each relevant column to create plots\n",
        "for i, column in enumerate(relevant_columns):\n",
        "    if i >= len(axs):  # If there are more columns than initially estimated subplots\n",
        "        break  # Avoid indexing errors on axs\n",
        "    # Determine if the column is numeric\n",
        "    if pd.api.types.is_numeric_dtype(df_with_sell_price[column]):\n",
        "        # Create scatter plot for numeric columns\n",
        "        sns.scatterplot(data=df_with_sell_price, x=column, y='sell_price', ax=axs[i])\n",
        "    else:\n",
        "        # Handle categorical data with fewer than 20 unique values\n",
        "        if df_with_sell_price[column].nunique() < 20:\n",
        "            sns.boxplot(x=column, y='sell_price', data=df_with_sell_price, ax=axs[i])\n",
        "        else:\n",
        "            # Indicate skipping the plot for high cardinality\n",
        "            axs[i].text(0.5, 0.5, 'Skipped: Too many categories', ha='center', va='center')\n",
        "            axs[i].set_title(f'Skipped plotting for {column}')\n",
        "            continue  # Skip further formatting for this subplot\n",
        "    axs[i].set_title(f'{column} vs. Sell Price')\n",
        "    axs[i].set_xlabel(column)\n",
        "    axs[i].set_ylabel('Sell Price')\n",
        "\n",
        "# Hide any unused subplots that may exist if the number of plots is less than the number of subplots\n",
        "for j in range(i + 1, len(axs)):\n",
        "    axs[j].set_visible(False)\n",
        "\n",
        "# Adjust layout and display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix of numeric features\n",
        "correlation_matrix = df_with_sell_price.select_dtypes(include=['float64', 'int64']).corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YtueDVXDuAo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for correlation calculations\n",
        "numeric_df = df_with_sell_price.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_corr = numeric_df.corr(method='pearson')['sell_price'].sort_values(ascending=False)\n",
        "\n",
        "# Calculate Spearman correlation coefficients for ordinal or non-linear relationships\n",
        "spearman_corr = numeric_df.corr(method='spearman')['sell_price'].sort_values(ascending=False)\n",
        "\n",
        "# Print the results\n",
        "print(\"Pearson Correlation Coefficients:\\n\", pearson_corr)\n",
        "print(\"\\nSpearman Correlation Coefficients:\\n\", spearman_corr)"
      ],
      "metadata": {
        "id": "rxFG74m0w2Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming df_with_sell_price is already defined and loaded\n",
        "\n",
        "# Select only numeric columns for correlation calculations\n",
        "numeric_df = df_with_sell_price.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Remove inf/-inf, replace with NaN\n",
        "numeric_df = numeric_df.replace([float('inf'), float('-inf')], pd.NA)\n",
        "\n",
        "# Initialize dictionaries to store correlations and p-values\n",
        "pearson_corr = {}\n",
        "pearson_p_values = {}\n",
        "spearman_corr = {}\n",
        "spearman_p_values = {}\n",
        "\n",
        "# Calculate Pearson and Spearman correlations and p-values for each numeric column against 'sell_price'\n",
        "for column in numeric_df.columns:\n",
        "    if column != 'sell_price':  # Skip comparing 'sell_price' with itself\n",
        "        # Drop NaN values specifically in the current column to keep maximum data\n",
        "        valid_data = numeric_df.dropna(subset=[column, 'sell_price'])\n",
        "\n",
        "        # Check if data has variability and enough points\n",
        "        if valid_data[column].std() != 0 and len(valid_data[column]) > 1:\n",
        "            # Pearson correlation\n",
        "            corr, p_value = stats.pearsonr(valid_data[column], valid_data['sell_price'])\n",
        "            pearson_corr[column] = corr\n",
        "            pearson_p_values[column] = p_value\n",
        "\n",
        "            # Spearman correlation\n",
        "            corr, p_value = stats.spearmanr(valid_data[column], valid_data['sell_price'])\n",
        "            spearman_corr[column] = corr\n",
        "            spearman_p_values[column] = p_value\n",
        "        else:\n",
        "            # Handle cases where there is no variability or insufficient data\n",
        "            pearson_corr[column] = None\n",
        "            pearson_p_values[column] = None\n",
        "            spearman_corr[column] = None\n",
        "            spearman_p_values[column] = None\n",
        "\n",
        "# Print Pearson results\n",
        "print(\"Pearson Correlation Coefficients and P-values:\")\n",
        "for col in pearson_corr:\n",
        "    print(f\"{col}: Correlation = {pearson_corr[col]}, P-value = {pearson_p_values[col]}\")\n",
        "\n",
        "# Print Spearman results\n",
        "print(\"\\nSpearman Correlation Coefficients and P-values:\")\n",
        "for col in spearman_corr:\n",
        "    print(f\"{col}: Correlation = {spearman_corr[col]}, P-value = {spearman_p_values[col]}\")\n"
      ],
      "metadata": {
        "id": "lOxQ6zT43-r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming df_with_sell_price is your DataFrame\n",
        "# Select only numeric columns for correlation calculations\n",
        "numeric_df = df_with_sell_price.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Remove inf/-inf, replace with NaN\n",
        "numeric_df = numeric_df.replace([float('inf'), float('-inf')], pd.NA)\n",
        "\n",
        "# Initialize dictionaries to store correlations and p-values\n",
        "pearson_corr = {}\n",
        "pearson_p_values = {}\n",
        "spearman_corr = {}\n",
        "spearman_p_values = {}\n",
        "\n",
        "# Define significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calculate Pearson and Spearman correlations and p-values for each numeric column against 'sell_price'\n",
        "for column in numeric_df.columns:\n",
        "    if column != 'sell_price':  # Skip comparing 'sell_price' with itself\n",
        "        # Drop NaN values specifically in the current column to keep maximum data\n",
        "        valid_data = numeric_df.dropna(subset=[column, 'sell_price'])\n",
        "\n",
        "        # Check if data has variability and enough points\n",
        "        if valid_data[column].std() != 0 and len(valid_data[column]) > 1:\n",
        "            # Pearson correlation\n",
        "            corr, p_value = stats.pearsonr(valid_data[column], valid_data['sell_price'])\n",
        "            if p_value < alpha:\n",
        "                pearson_corr[column] = corr\n",
        "                pearson_p_values[column] = p_value\n",
        "\n",
        "            # Spearman correlation\n",
        "            corr, p_value = stats.spearmanr(valid_data[column], valid_data['sell_price'])\n",
        "            if p_value < alpha:\n",
        "                spearman_corr[column] = corr\n",
        "                spearman_p_values[column] = p_value\n",
        "\n",
        "# Print Pearson results for significant correlations\n",
        "print(\"Significant Pearson Correlation Coefficients and P-values:\")\n",
        "for col in pearson_corr:\n",
        "    print(f\"{col}: Correlation = {pearson_corr[col]:.3f}, P-value = {pearson_p_values[col]:.4f}\")\n",
        "\n",
        "# Print Spearman results for significant correlations\n",
        "print(\"\\nSignificant Spearman Correlation Coefficients and P-values:\")\n",
        "for col in spearman_corr:\n",
        "    print(f\"{col}: Correlation = {spearman_corr[col]:.3f}, P-value = {spearman_p_values[col]:.4f}\")\n"
      ],
      "metadata": {
        "id": "vKlH_8bx4Yyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import category_encoders as ce  # Ensuring correct import of category_encoders\n",
        "\n",
        "# Assuming df is your DataFrame and you've already installed category_encoders using !pip install category_encoders\n",
        "\n",
        "# Create a copy of final_df to work with, ensuring we don't modify the original DataFrame\n",
        "final_df_combined = final_df.copy()\n",
        "\n",
        "# Create geographic clusters\n",
        "coords = final_df_combined[['latitude', 'longitude']]\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "final_df_combined['geo_cluster'] = kmeans.fit_predict(coords)\n",
        "\n",
        "# Use postcode directly (ensure it's treated as a categorical variable)\n",
        "final_df_combined['postcode'] = final_df_combined['postcode'].astype(str)\n",
        "\n",
        "# Using TargetEncoder to encode high-cardinality categorical features\n",
        "encoder = ce.TargetEncoder()\n",
        "final_df_combined['legal_district_encoded'] = encoder.fit_transform(final_df_combined['legal_district'], final_df_combined['sell_price'])\n",
        "final_df_combined['locality_encoded'] = encoder.fit_transform(final_df_combined['locality'], final_df_combined['sell_price'])\n",
        "final_df_combined['county_encoded'] = encoder.fit_transform(final_df_combined['brokers_description'], final_df_combined['sell_price'])  # brokers_description is county\n",
        "\n",
        "# Now, final_df_combined contains the original data along with the new features.\n",
        "print(final_df_combined.head())  # Check the first few rows of the new DataFrame\n"
      ],
      "metadata": {
        "id": "1WREwdavpODo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Total Property Area\n",
        "final_df_combined['total_property_area'] = final_df['living_area'] + final_df['additional_area'] + final_df['plot_area']\n",
        "\n",
        "# Calculate Room Density - average area per room\n",
        "final_df_combined['avg_room_area'] = final_df_combined['living_area'] / final_df_combined['rooms']\n",
        "\n",
        "# Calculate the volume of the property if width and height are available\n",
        "final_df_combined['property_volume'] = final_df_combined['living_area'] * final_df_combined['width'] * final_df_combined['height']\n",
        "\n",
        "# Create Price Per Square Meter\n",
        "final_df_combined['price_per_sqm'] = final_df['asking_price'] / final_df['living_area']\n",
        "\n",
        "# Create Rent Per Square Meter\n",
        "final_df['rent_per_sqm'] = final_df['rent'] / final_df['living_area']\n",
        "# Calculate cost to benefit ratios\n",
        "final_df_combined['rent_to_area_ratio'] = final_df_combined['rent'] / final_df_combined['total_area']\n",
        "final_df_combined['operating_cost_to_living_area_ratio'] = final_df_combined['operating_cost'] / final_df_combined['living_area']\n",
        "\n",
        "# Categorizing floors into low, mid, and high can sometimes be useful:\n",
        "final_df_combined['floor_category'] = pd.cut(final_df_combined['floor'], bins=[0, 5, 10, 100], labels=['Low', 'Mid', 'High'], right=False)\n",
        "\n",
        "# Create Cost Efficiency Ratio\n",
        "final_df_combined['cost_efficiency_ratio'] = final_df['operating_cost'] / final_df['rent']\n",
        "\n",
        "# Check the first few rows of the new DataFrame\n",
        "print(final_df_combined.head())\n"
      ],
      "metadata": {
        "id": "8RQvkn9CuokQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_combined['luxury_score'] = (\n",
        "    final_df_combined['has_balcony'] +\n",
        "    final_df_combined['has_fireplace'] +\n",
        "    final_df_combined['has_patio']\n",
        ")\n",
        "\n",
        "# Calculate a 'sustainability_score', starting with solar panels\n",
        "final_df_combined['sustainability_score'] = final_df_combined['has_solar_panels']\n",
        "\n",
        "# Optionally, if you believe patio and balcony contribute to sustainability:\n",
        "final_df_combined['sustainability_score'] += (\n",
        "    final_df_combined['has_patio'] +\n",
        "    final_df_combined['has_balcony']\n",
        ")\n",
        "\n",
        "# Check the first few rows of the new DataFrame to verify the additions\n",
        "print(final_df_combined[['luxury_score', 'sustainability_score']].head())"
      ],
      "metadata": {
        "id": "tnMI4_JBp6Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'final_df_combined' is loaded and includes all necessary columns\n",
        "\n",
        "# Cast identifiers to string to ensure they are treated as categorical\n",
        "final_df_combined['housing_coop_id'] = final_df_combined['housing_coop_id'].astype(str)\n",
        "final_df_combined['agency_id'] = final_df_combined['agency_id'].astype(str)\n",
        "final_df_combined['housing_association_org_number'] = final_df_combined['housing_association_org_number'].astype(str)\n",
        "\n",
        "# Initialize the encoder\n",
        "encoder = ce.TargetEncoder()\n",
        "\n",
        "# Apply Target Encoding\n",
        "final_df_combined['housing_coop_encoded'] = encoder.fit_transform(final_df_combined['housing_coop_id'], final_df_combined['sell_price'])\n",
        "final_df_combined['agency_id_encoded'] = encoder.fit_transform(final_df_combined['agency_id'], final_df_combined['sell_price'])\n",
        "final_df_combined['housing_association_encoded'] = encoder.fit_transform(final_df_combined['housing_association_org_number'], final_df_combined['sell_price'])\n",
        "\n",
        "# Use asking price directly and create a feature for the asking price ratio compared to the median\n",
        "median_asking_price = final_df_combined['asking_price'].median()\n",
        "final_df_combined['asking_price_ratio'] = final_df_combined['asking_price'] / median_asking_price\n",
        "\n",
        "# Aggregate metrics for housing associations\n",
        "final_df_combined['total_units'] = final_df_combined['number_of_rental_units'] + final_df_combined['number_of_units']\n",
        "final_df_combined['total_area'] = final_df_combined['total_commercial_area'] + final_df_combined['total_living_area'] + final_df_combined['total_plot_area'] + final_df_combined['total_rental_area']\n",
        "final_df_combined['residential_to_commercial_ratio'] = final_df_combined['total_living_area'] / final_df_combined['total_commercial_area'].replace(0, 1)  # Avoid division by zero\n"
      ],
      "metadata": {
        "id": "DAkPAXHKwEFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Assuming 'final_df_combined' is your DataFrame and it's already loaded with the necessary columns\n",
        "\n",
        "# Current year or year sold can be dynamic or fixed based on your dataset\n",
        "current_year = datetime.now().year\n",
        "\n",
        "# Calculate property age\n",
        "final_df_combined['property_age'] = current_year - final_df_combined['construction_year']\n",
        "\n",
        "# Create age categories\n",
        "bins = [0, 5, 10, 20, 30, 50, 100, float('inf')]\n",
        "labels = ['0-5', '6-10', '11-20', '21-30', '31-50', '51-100', '100+']\n",
        "final_df_combined['age_category'] = pd.cut(final_df_combined['property_age'], bins=bins, labels=labels)\n",
        "\n",
        "# Interaction feature between new construction status and age\n",
        "# Negate the age if it is new construction to show it as a premium property\n",
        "final_df_combined['age_new_construction_interaction'] = final_df_combined.apply(\n",
        "    lambda row: -row['property_age'] if row['is_new_construction'] else row['property_age'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Ensure that 'is_new_construction' is treated as a boolean if it's not already\n",
        "# This step assumes 'is_new_construction' may be in a non-boolean format such as int or string\n",
        "if final_df_combined['is_new_construction'].dtype != 'bool':\n",
        "    final_df_combined['is_new_construction'] = final_df_combined['is_new_construction'].astype(bool)\n",
        "\n",
        "# Print the DataFrame to check the new columns\n",
        "print(final_df_combined[['property_age', 'age_category', 'age_new_construction_interaction']].head())\n"
      ],
      "metadata": {
        "id": "s38Bs0TFxn_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_combined.head()\n",
        "final_df_combined.to_excel('final_df_combined.xlsx', index=False, engine='openpyxl')"
      ],
      "metadata": {
        "id": "jrt2n1dQwUOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}